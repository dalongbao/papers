how does it decide how many tokens to generate?

it *is* learning, but it's generating 8 tokens? next token only?

which token is it actually predicting? what? 

i think for more text i need to implement it myself

this raises the question of how tokens are added to LLM generation that i don't quite understand yet
1. sequence is parsed
2. the output is added
3. read again?

